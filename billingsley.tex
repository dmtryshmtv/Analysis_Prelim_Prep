\documentclass[avery5388,grid,frame]{flashcards}

\cardfrontstyle[\large\slshape]{headings}
\cardbackstyle{empty}

\usepackage{amssymb, amsmath, amsfonts}
\usepackage{mathtools}
\usepackage{physics}
\usepackage{enumerate}
\usepackage{array}
\usepackage{enumitem}
\usepackage{hyperref}


\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\rar}{\rightarrow}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Oc}{\mathcal{O}}
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
%\newcommand{\alert}[1]{\textcolor{red}{#1}}
\newcommand{\Var}{\text{Var}}
\newcommand{\sigf}{\sigma-\text{field}}
\newcommand{\F}{\mathcal F}


\begin{document}
\cardfrontfoot{Billingsley}


\begin{flashcard}
    {Define a field and a $\sigf$}
    A field on a set $\Omega$ is a collection of subsets $\F$ such that:
    \begin{enumerate}
        \item (at least contains two sets) $\emptyset, \Omega \in \F$,
        \item (closer under complement) if $A \in \F$, then $A^c \in \F$,
        \item (closure under finite union) if $A, B \in \F$, then $A \cup B \in \F$
    \end{enumerate}

    A $\sigf$ $\F$ on the set $\Omega$ also has:
    \begin{enumerate}[resume]
        \item (closure under countable union) if $A_1, \dots \in \F$, then $\cup_i A_i \in \F$.
    \end{enumerate}
\end{flashcard}


\begin{flashcard}
    {Define a probability measure and give some of its properties}
    A probability measure on a set $\Omega$ with field $\F$ is a function $P: \F \rightarrow [0, \infty)$ with:
    \begin{enumerate}
        \item $0 \leq P(A) \leq 1, \forall A \in \F$,
        \item $P(\emptyset) = 0$ and $P(\Omega) = 1$,
        \item if $A_1, \dots$ are disjoint and $\cup_i A_i \in \F$, then
        $$P(\cup_i A_i) = \sum_i P(A_i)$$
    \end{enumerate}
\end{flashcard}


\begin{flashcard}
    {List some properties of probability measures}
    A probability measure $P$ on $\Omega$ with field $\F$ has
    \begin{enumerate}
        \item (monotonicity), if $A \subset B$, then $P(A) \leq P(B)$,
        \item (inclusion-exclusion) $P(A \cup B) = P(A) + P(B) - P(A \cap B)$ and more generally
        $$P(\cup A_n) = \sum_i P(A_i) - \sum_{i < j} P(A_i \cap A_j) + \sum_{i < j < k} P(A_i \cap A_j \cap A_k)$$
        $$+ \dots + (-1)^{n+1} P(A_1 \cap \dots A_n),$$
        \item (countably subadditive) if $A_1, \dots \in \F$ and $\cup_i A_i \in \F$, then $P(\cup_i A_i) \leq \sum_i P(A_i)$,
        \item (continuous from below) if $A_1 \subset A_2 \dots \subset A$, then $P(A_n) \uparrow P(A)$
        \item (continuous from above) if $A_1 \supset A_2 \dots \supset A$, then $P(A_n) \downarrow P(A)$
    \end{enumerate}
\end{flashcard}


\begin{flashcard}
    {Briefly describe the process of showing the Lebesgue measure exists}
    Theorem 3.1 (Cartheodory extension theorem): a probability measure on a field can be uniquely extended to the generated $\sigf$ if the measure is $\sigma$-finite.

    Hence, to construct the Lebesgue measure, first we define the Lebesgue measure that assigns to half-open intervals the interval length, second we verify that this is a well-defined measure on the Borel field, and then we apply theorem 3.1.

    Proving theorem 3.1 is involved. Also, studying $\sigma(\mathcal B_0)$ is necessary, where $\mathcal B_0$ is the field of finite unions and intersections of intervals.
\end{flashcard}


\begin{flashcard}
    {Define $\lim \sup_n A_n$ and $\lim \inf_n A_n$ of sets and give an interpretation and an inequality}
    $\lim \sup_n A_n = \cup_n \cap_{k \geq n} A_k$. If $w$ is in LHS, then for every $n$, there exists some $k \geq n$ so that $w \in A_k$, hence $w$ is in infinitely many of the $A_n$. ``Infinitely often''.

    $\lim \inf_n A_n = \cap_n \cup_{k \geq n} A_k$. If $w$ is in LHS, then there exists $n$ such that for all $k \geq n$, $w \in A_k$ for all $k$. Hence, $w$ is in all but finitely many $A_n$. ``Eventually''.

    $$P(\lim \inf_n A_n) \leq \lim \inf_n P(A_n)$$
    $$\leq \lim \sup_n P(A_n) \leq P(\lim \sup_n A_n)$$
\end{flashcard}


\begin{flashcard}
    {Define measure-theoretic independence (events, collections of events, $\sigf$)}
    \begin{itemize}
        \item Two events $A$ and $B$ are independent if $P(A \cap B) = P(A) P(B)$.
        \item A collection of events $\{ A_1, \dots, A_n \}$ are independent if
        $$P(A_{k_1} \cap \dots A_{k_j}) = P(A_{k_1}) \dots P(A_{k_j})$$
        for all $2 \leq j \leq n$ and $1 \leq k_1 < \dots < k_n \leq n$.
        \item A collection of classes $\mathcal A_1, \dots, \mathcal A_n$ in a $\sigf$ $\F$ are independent if for each choice of $A_i \in \mathcal A_i$, the collection $\{ A_n \}$ is independent.
        \item Two $\sigf$s $\mathcal A$ and $\mathcal B$ are independent if for every $A \in \mathcal A$ and $B \in \mathcal B$, we have $\mu(A \cap B) = \mu(A) \mu(B)$.
    \end{itemize}
\end{flashcard}


\begin{flashcard}
    {State the Borel-Cantelli Lemma 1 and proof}
    If $\sum P(A_n) < \infty$, then $P(\lim \sup_n A_n) = 0$.

    \emph{Proof:} Observe that $\lim \sup_n A_n \subset \cup_{k \geq m} A_k$ for all $m$.
    This implies that
    $$P(\lim \sup_n A_n) \leq P(\cup_{k \geq m} A_k) \leq \sum_{k \geq m} P(A_k).$$
    Since this holds for arbitrary $m$ and the right hand side sum goes to $0$ if the infinite sum converges, the lemma follows.
\end{flashcard}


\begin{flashcard}
    {State the Borel-Cantelli Lemma 2 and proof}
    If $\{ A_n \}$ are independent and $\sum P(A_n) = \infty$ then $P(\lim \sup_n A_n) = 1$.

    \emph{Proof:} It is enough to show that $P(\cup_n \cap_{k \geq n} A_k^c) = 0$ for which it is enough to show that $P(\cap_{k \geq n} A_k^c) = 0$ for all $k$. Note that $1 - x \leq e^{-x}$, then (by independence)
    $$P(\cap_{k = n}^{n + j} A_k^c) = \prod_{k = n}^{n+j} 1 - P(A_k) \leq \exp \{ - \sum_{k = n}^{n + j} P(A_k) \}.$$
    But since the sum diverges, as $j \rightarrow \infty$, the RHS goes to 0, hence
    $$P( \cap_{k=n}^\infty A_k^c ) = \lim_j P(\cap_{k=n}^{n+j} A_k^c) = 0$$
\end{flashcard}

\begin{flashcard}
    {What's a use case of the Borel-Cantelli lemmas (both)?}
\end{flashcard}


\begin{flashcard}
    {Define tail $\sigf$ and state the Kolmogorov 0-1 law}
    Given a sequence of events $A_1, A_2, \dots$ in a probability space $(\Omega, \F, P)$, the tail $\sigf$ associated with the sequence $\{ A_n \}$ is
    $$\mathcal T = \cap_{n=1}^\infty \sigma(A_n, A_{n+1}, \dots).$$
    The $\lim \sup_n A_n$ and $\lim \inf_n A_n$ are events in the tail $\sigf$.

    The Kolmogorov zero-one law: if $A_1, A_2, \dots$ are independent, then for each event $A$ in the tail $\sigf$, $P(A)$ is either 0 or 1.
\end{flashcard}


\begin{flashcard}
    {What's a use case of the 0-1 law?}
\end{flashcard}


\begin{flashcard}
    {Simple random variables}
    A random variables $X$ on $(\Omega, \F)$ is simple iff it can be written as
    $$X(w) = \sum_i x_i I_{A_i}$$
    for some finite set of $x_i$ and $A_i \in \F$.

    Simple random variables $X_n$ converge to $X$ with probability 1 ($\lim_n X_n = X$) iff $\forall \epsilon > 0$,
    $$P(|X_n - X| > \epsilon \text{ i.o.}) = 0$$
    which, if the above holds, implies that
    $$\lim_n P(|X_n - X| > \epsilon) = 0.$$

    Note that
    $$\{ \lim_n X_n = X \}^c = \cup_\epsilon \{ | X_n - X | \geq \epsilon \text{ i.o.} \} = \cup_\epsilon \cup_n \cap_{k \geq n} \{ |X_n - X| \geq \epsilon \}.$$
\end{flashcard}


\begin{flashcard}
    {State and prove the Markov's inequality}
    Markov's inequality: For a random variable $X$, nonnegative, then for positive $\alpha$, we have
    $$P(X \geq \alpha) \leq \frac 1 \alpha \E[X].$$

    \emph{Proof:} Note that for any convex $f$ and any set $A$, we have that
    $$\min_{x \in A} f(x) \mathbf 1_{A} \leq E[X \mathbf 1_{A}] \leq E[X]$$

    Hence, with $f(x) = x$ and $A = [\alpha,\infty)$, the result follows. If we use $f(x) = |x|^k$, then we have for positive $\alpha$:
    $$\Pr(|X| \geq \alpha) \leq \frac 1 \alpha^k \E[|X|^k]$$
\end{flashcard}


\begin{flashcard}
    {What's a use case of Markov's inequality?}
\end{flashcard}


\begin{flashcard}
    {State and prove Chebyshev's inequality}
    Chebyshev's inequality: for a random variable $X$, we have
    $$\Pr(|X - m| \geq \alpha) \leq \frac 1 \alpha^2 \Var(X)$$

    \emph{Proof:} Applying Markov's inequality with $k=2$ and subtracting $m = \E[X]$, we obtain the desired result.
\end{flashcard}


\begin{flashcard}
    {What's a use case of Chebyshev's inequality?}
\end{flashcard}


\begin{flashcard}
    {State and prove Jensen's inequality (finite case).}
    Jensen's inequality says that for a convex function $\phi(x)$ and a random variable $X$, we have
    $$\E[\phi(X)] \geq \phi(\E[X])$$

    \emph{Proof:} the proof follows by induction (base case follows form convexity; induction step follows from grouping $n$ of the sum terms together).

    \href{https://en.wikipedia.org/wiki/Jensen%27s_inequality#Proofs}{More details here.}
\end{flashcard}


\begin{flashcard}
    {What's a use case of Jensen's inequality?}
    Markov's inequality. Non-negativity of the Kullback-Leibler divergence and hence mutual information. The fact that $H(X) \leq \log | \mathcal X |$. The fact that $H(X | Y) \leq H(X)$. Independence bound on entropy (entropy of a collection of random variables is bounded above by the sum of their individual entropies).
\end{flashcard}


\begin{flashcard}
    {State and prove Holder's inequality.}
    Suppose that $\frac 1 p + \frac 1 q = 1$ for $p,q>1$. Then:
    $$\E[|XY|] \leq \E[|X|^p]^{\frac 1 p} \E[|Y|^q]^{\frac 1 q}$$

    \emph{Proof:} Young's. \href{https://en.wikipedia.org/wiki/H%C3%B6lder%27s_inequality#Proof_of_H%C3%B6lder's_inequality}{Here.}
\end{flashcard}


\begin{flashcard}
    {What's a use case of Holder's inequality?}
\end{flashcard}


\begin{flashcard}
    {State and prove the strong law of large numbers.}
    If $X_n$ are iid and $\E[X_n] = m$, then
    $$\Pr(\lim_n n^{-1} S_n = m) = 1$$

    \emph{Proof:} WLOG $m=0$. It is enough to show that $\Pr(|n^{-1} S_n| \geq \epsilon \text{ i.o}) = 0$ for each $\epsilon$.

    Let $\E[X_i^2] = \sigma^2$ and $\E[X_i^4] = \xi^4$. By independence, we have
    $$\E[S_n^4] = n \xi^4 + 3n(n-1) \sigma^4 \leq K n^2$$
    where $K$ does not depend on $n$. By Markov's inequality for $k=4$,
    $$\Pr(|S_n| \geq n \epsilon) \leq K n^{-2} \epsilon^{-4},$$
    so the result follows by the first Borel-Cantelli lemma (the event probs are summable, hence the lim sup is $0$).
\end{flashcard}


\begin{flashcard}
    {State and prove the weak law of large numbers.}
    If $X_n$ are iid and $\E[X_n] = m$, then for all $\epsilon$
    $$\lim_n \Pr(| n^{-1} S_n - m| \geq \epsilon) = 0.$$

    \emph{Proof:} By appealing to the strong law, we have
    $$\Pr(| n^{-1} S_n - m| \geq \epsilon) \leq \frac{\Var(S_n)}{n^2 \epsilon^2} = \frac{n \Var(X_1)}{n^2 \epsilon^2} \rightarrow 0.$$
\end{flashcard}


\begin{flashcard}
    {Demonstrate a use of the strong law.}
\end{flashcard}


\begin{flashcard}
    {Demonstrate a use of the weak law.}
\end{flashcard}


\begin{flashcard}
    {What is a measurable mapping? What are some properties?}
    For two measure spaces $(\Omega, \F)$ and $(\Omega', \F')$, a transformation $T: \Omega \rightarrow \Omega'$ is measurable $\F / \F'$ iff for all $A \in \F'$, $T^{-1}(A) \in \F$.

    If $T^{-1}(A) \in \F$ for each $A \in \mathcal A$, where $\mathcal A$ generates $\F'$, then $T$ is $\F / \F'$ measurable.

    A random vector is measurable iff each component function is measurable.

    Continuous functions are measurable. If $f_k: \Omega \rightarrow \R$ are measurable $\F$, then $g(f_1(w),\dots,f_k(w))$ is measurable $\F$ if $g: \R^k \rightarrow \R$ is measurable.

    Composition of measurable functions is measurable. Sum, sup, lim sup, product are measure-preserving. A limit of measurable functions is measurable if the limit exists everywhere. We can construct a sequence of simple measurable functions that increase to any given measurable function.
\end{flashcard}


\begin{flashcard}
    {What is the pushforward measure?}
    Given $(\Omega, \F)$ and $(\Omega',\F')$ and a measurable transformation $T:\Omega \rightarrow \Omega'$ and a measure $\mu$ on $\F$, then $\mu T^{-1}(A') = \mu (T^{-1}(A'))$ is a pushforward measure on $\F'$.
\end{flashcard}


\begin{flashcard}
    {What is the change-of-variables formula?}
    A measurable function $g$ on $\Omega'$ is integrable with respect to the pushforward measure $\mu T^{-1} = T(\mu)$ iff the composition $g \circ T$ is integrable with respect to the measure $\mu$. In that case,
    $$\int_{\Omega'} g d(\mu T^{-1}) = \int_{\Omega} g \circ T d\mu$$
\end{flashcard}


\begin{flashcard}
    {Say a few things about distribution functions.}
    A distribution function for a random variable $X$ on $\R$ is $F(x) = \Pr(X \leq x)$. It is non-decreasing, right-continuous (by continuity from above). By continuity from below, $\lim_{y \uparrow x} F(y) = F(x^-) = \Pr(X < x)$.

    For every non-decreasing, right-continuous function with $\lim_{x \rightarrow -\infty} F(x) = 0$ and $\lim_{x \rightarrow \infty} F(x) = 1$, there exists on some probability space a random variable $X$ for $F$ is the distribution function.

    If $\lim_n F_n(x) = F(x)$ for all $x$, then we write $F_n \implies F$ and say that the distributions converge weakly and their corresponding random variables converge weakly.
\end{flashcard}


\begin{flashcard}
    {How is the integral defined for a non-negative measurable function? Properties?}
    $\int_\Omega f d\mu = \sup \sum_i (\inf_{w \in A_i} f(w)) \mu(A_i)$ where the sup is taken over all partitions of $\Omega$.

    If $f \leq g$ then $\int f \leq \int g$.

    If $f_n \uparrow f$ then $\int f_n \uparrow \int f$.

    The integral is linear.

    If $f = 0$ a.e., then $\int f = 0$. If the measure of the set where $f$ is non-zero is positive, then the integral is positive. If the integral exists, then $f < \infty$ a.e.
\end{flashcard}


\begin{flashcard}
    {State some properties of the general integral (addition, limits)}
    \begin{itemize}
        \item Monotonicity: (i) if $f \leq g$ and are integrable, then $\int f d\mu \leq \int g d\mu$,
        \item Linearity: if $f,g$ are integrable, then $\int (\alpha f + \beta g) d\mu \leq \alpha \int f d\mu + \beta \int g d\mu$.
        \item Monotone convergence: if $0 \leq f_n \uparrow f$ a.e., then $\int f_n d\mu \uparrow \int f d\mu$,
        \item Fatou's lemma: for non-negative $f_n$, $\int \lim \inf f_n d\mu \leq \lim \inf \int f_n d\mu$,
        \item Dominated convergence: if $|f_n| \leq g$ a.e., where $g$ is integrable, and if $f_n \rightarrow f$ a.e., then $f$ and $f_n$ are integrable and $\int f_n d\mu \rightarrow \int f d\mu$.
    \end{itemize}
\end{flashcard}


\begin{flashcard}
    {Give an application of MCT.}
    Consider the space $\{ 1, 2, \dots \}$ with the counting measure. If for all $m$, we have $0 \leq x_{n,m} \uparrow x_m$ as $n \rightarrow \infty$, then $\lim_n \sum_m x_{n,m} = \sum_m x_m$.

    For an infinite sequence of measures $\mu_n$ on $\F$, $\mu(A) = \sum_n \mu_n(A)$ defines another measure (countably additive because sums can be reversed in a nonnegative double series). You can show that $\int f d\mu = \sum_n \int f d\mu_n$ holds for all nonnegative $f$.
\end{flashcard}


\begin{flashcard}
    {Give an application of DOM.}
    Consider the sequence $f_n = X 1_{A_n} $ where $A_n \downarrow A$ with $\mu(A) = 0$ and $A_1 = \Omega$. Assuming that $f_1$ is absolutely integrable, note that $|f_1| \geq |f_n|$ hence the sequence is dominated and therefore $\lim_n \int f_n d\mu = \int X 1_{A} d\mu = 0$.
\end{flashcard}


\begin{flashcard}
    {Give an application of Fatou's lemma.}
    Consider on $(\R, \mathcal R, \lambda)$ the functions $f_n = n^2 I_{(0,n^{-1})}$ and $f = 0$ satisfy $f_n \rightarrow 0$ for each $x$, but $\int f d\lambda = 0$ and $\int f_n d\lambda = n \rightarrow \infty$. Hence Fatou's lemma inequality can be strict. Note that DOM and MCT do not apply here, as $f_n$ are unbounded.
\end{flashcard}


\begin{flashcard}
    {What is the continuous mapping theorem?}
    A sequence of random variables $X_n$ converging in distribution/probability/as to $X$ implies that $g(X_n)$ converges to $g(X)$ if $g$ is continuous.

    If $g$ is just a function on $\R \rightarrow \R$ and it's a composition, this is fine. However, if you want this to state something about the expectation, then $g(X) \coloneqq \E[X^k]$ (so $g$ is now an operator), this is only continuous if the moments are bounded. However even in a space of bounded moment random variables (aka. $L^p(X)$), one can find a sequence of random variables whose integrals will blow up, so this $g$ is not a bounded operator, hence not continuous.

    If random variables converge in distribution, then do their moments converge? No:
    $Pr(X_n = n^2) = 1/n, Pr(X_n = 0) = 1 - 1/n.$
\end{flashcard}


\begin{flashcard}
    {When do the integral values determine the integrands?}
    If $f$ and $g$ are nonnegative and $\int_A f d\mu = \int_A g d\mu$ for all $A \in \F$ and if $\mu$ is $\sigf$ then $f = g$ a.e.

    If $f$ and $g$ are integrable and $\int_A f d\mu = \int_A g d\mu$ for all $A \in \F$ then $f = g$ a.e.
\end{flashcard}


\begin{flashcard}
    {What is a density?}
    $\delta$ is a density if it is a nonnegative function so that for two measures $\nu$ and $\mu$ we have
    $$\nu(A) = \int_A \delta d\mu, \quad A \in \F$$
\end{flashcard}


\begin{flashcard}
    {What is Scheffe's theorem?}
    Suppose $\nu_n(A) = \int_A \delta_n d\mu$ and $\nu(A) = \int_A \delta d\mu$ for densities $\delta_n$ and $\delta$. If
    $$\nu_n(\Omega) = \nu(\Omega)< \infty, \quad n = 1,2\dots,$$
    and if $\delta_n \rightarrow \delta$ except on a set of $\mu$-measure 0, then
    $$\sup_{A \in \F} | \nu(A) - \nu_n(A) | \leq \int_\Omega |\delta - \delta_n| d\mu \rightarrow 0$$

    In other words, the total variation of two measures is bounded above by the $L^1$ difference of the densities, hence if the densities convergence, then the measures converge.
\end{flashcard}


\begin{flashcard}
    {What is uniform integrability?}
    A sequence $f_n$ is uniformly integrable if
    $$\lim_{\alpha \rightarrow \infty} \sup_n \int_{[|f_n| \geq \alpha]} |f_n| d\mu = 0$$
\end{flashcard}


\begin{flashcard}
    {What is an application of uniform integrablity?}
    Suppose $\mu(\Omega) < \infty$ and $f_n \rightarrow f$ a.e.

    (i) if $f_n$ are uniformly integrable, then $f$ is integrable and
    $$\int f_n d\mu \rightarrow \int f d\mu$$

    (ii) if $f$ and $f_n$ are nonnegative and integrable, then the conclusion of (i) implies that $f_n$ are uniformly integrable.

    If $\mu(\Omega) < \infty$, $f$ and $f_n$ are integrable, and $f_n \rightarrow f$ a.e., then the following are equivalent:
    \begin{itemize}
        \item $f_n$ are uniformly integrable,
        \item $\int | f - f_n | d\mu \rightarrow 0$,
        \item $\int |f_n| d\mu \rightarrow \int |f| d\mu$.
    \end{itemize}
\end{flashcard}


\begin{flashcard}
    {What is Riemann integrability?}
    A real function $f$ on $(a,b]$ is Riemann integrable with integral $r$ if: for all $\epsilon$ there exists a $\delta$ with
    $$| r - \sum_i f(x_i) \lambda(I_i) | < \epsilon$$
    if $\{ I_i \}$ is any finite partition of $(a,b]$ into subintervals satisfying $\lambda(I_i) < \delta$ and if $x_i \in I_i$ for each $i$.
\end{flashcard}


\begin{flashcard}
    {What is a product space? What is the product measure?}
    Given two measure spaces $(X,\mathcal X)$ and $(Y,\mathcal Y)$, the product space is $(X \times Y, \mathcal X \times \mathcal Y)$ where $\mathcal X \times \mathcal Y$ is the set of sets $A \times B$ where $A \in \mathcal X$ and $B \in \mathcal Y$.

    Given two measures $\mu, \nu$, the product measure is a measure $\pi$ with $\pi(A \times B) = \mu(A) \nu(B)$. This measure is unique.
\end{flashcard}


\begin{flashcard}
    {What is Fubini's theorem?}
    If $(X, \mathcal X, \mu)$ and $(Y, \mathcal Y, \vu)$ are $\sigf$ measure spaces, then
    $$\int_{X \times Y} f(x,y) \pi(d(x,y)) = \int_X \left [ \int_Y f(x,y) \nu(dy) \right ] \mu(dx)$$
    $$\int_{X \times Y} f(x,y) \pi(d(x,y)) = \int_Y \left [ \int_X f(x,y) \nu(dx) \right ] \mu(dy)$$
\end{flashcard}


\begin{flashcard}
    {What is convergence in probability?}
    Random variables $X_n$ converge in probability to $X$, written $X_n \rightarrow_P X$ if for each positive $\epsilon$
    $$\lim_n P(|X_n - X| \geq \epsilon) = 0$$

    A necessary and sufficient condition for convergence in probability is that each subsequence $X_{n_i}$ contain a further subsequence $X_{n_{k(i)}}$ such that $X_{n_{k(i)}} \rightarrow X$ with probability 1 as $i \rightarrow \infty$.

    In nonprobabilistic contexts, convergence in probability becomes convergence in measure.
\end{flashcard}


\begin{flashcard}
    {What is the Glivenko-Cantelli theorem?}
    Suppose that $X_1, X_2, \dots$ are independent and have a common distribution function $F$; put $D_n(w) = \sup_x |F_n(x, w) - F(x)|$, where
    $$F_n(x,w) = \frac 1 n \sum_{i=1}^n I_{(-\infty,x]}(X_i(w))$$
    Then $D_n \rightarrow 0$ with probability 1.
\end{flashcard}


\begin{flashcard}
    {What is the definition of expected value? Same some things about it and limits.}
    $\E[X] = \int X dP = \int_\Omega X(w) P(dw).$

    $\E[X 1_A] = \int_A X dP$.

    $\E[g(X)] = \int_\Omega g(X(w)) P(dw)$.

    Absolute moments $\E[|X|^k] = \int_{-\infty}^\infty |x|^k P(dx)$.

    By the way $P(dx)$ intuitively refers to the $P$ measure of a small change in $x$.

    If $X_n$ are dominated by an integrable random variable (or uniformly integrable), then $\E[X_n] \rightarrow \E[X]$ follows if $X_n \rightarrow_P X$.
\end{flashcard}


\begin{flashcard}
    {What is the MGF? What is it for normal, exponential, Poisson, and Binomial dists?}
    The MGF is
    $$M_X(t) = \E[e^{tX}] = \int_{-\infty}^\infty e^{sx} \mu(dx)$$

    The standard normal distribution: $M_X(t) = e^{t^2/2}$.

    The exponential: $M_X(t) = \frac{\alpha}{\alpha - s}$ defined for $s < \alpha$.

    The Poisson: $M_X(t) = e^{\lambda (e^t - 1)}$.

    The Binomial: $M_X(t) = p e^t + (1-p)$.
\end{flashcard}


\begin{flashcard}
    {Roughly sketch the proof of the strong law of large numbers from first principles.}
    You only need to show it for nonnegative rvs, by $\E[X_1^+] - \E[X_1^-] = \E[X]$.

    Truncated random variables and two applications of Borel-Cantelli and some other things.

\end{flashcard}


\begin{flashcard}
    {The weak law of large numbers? State and prove it.}
    $n^{-1} S_n \rightarrow_P \E[X_1]$.

    This follows in the same condition from the strong law, because a.s. convergence implies convergence in probability.

    A distribution concentrated on $[0,\infty)$ is determined by its MGF or its Laplace transform.
\end{flashcard}


\begin{flashcard}
    {}
\end{flashcard}


\begin{flashcard}
    {}
\end{flashcard}


\begin{flashcard}
    {}
\end{flashcard}


\begin{flashcard}
    {}
\end{flashcard}


\begin{flashcard}
    {}
\end{flashcard}


\begin{flashcard}
    {}
\end{flashcard}


\begin{flashcard}
    {}
\end{flashcard}


\end{document}
