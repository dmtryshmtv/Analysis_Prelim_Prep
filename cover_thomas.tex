\documentclass[avery5388,grid,frame]{flashcards}

\cardfrontstyle[\large\slshape]{headings}
\cardbackstyle{empty}

\usepackage{amssymb, amsmath, amsfonts}
\usepackage{mathtools}
\usepackage{physics}
\usepackage{enumerate}
\usepackage{array}
\usepackage{enumitem}


\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\rar}{\rightarrow}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Oc}{\mathcal{O}}
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
%\newcommand{\alert}[1]{\textcolor{red}{#1}}
\newcommand{\Var}{\text{Var}}
\newcommand{\sigf}{\sigma-\text{field}}
\newcommand{\F}{\mathcal F}


\begin{document}
\cardfrontfoot{Cover and Thomas}


\begin{flashcard}
    {Define entropy and list some properties}
    For a finite-valued random variable $X$, $H(X) = - \sum_i p(x_i) \log p(x_i)$.

    \begin{enumerate}
        \item Entropy is non-negative $H(X) \geq 0$ (for finite-ranged rvs).
        \item Entropy is bounded by the log of the size of the range $H(X) \leq \log |\mathcal X|$.
        \item It is concave: $H(\lambda p_1(x) + (1-\lambda) p_2(x)) \geq \lambda H(p_1(x)) + (1- \lambda) H(p_2(x))$.
    \end{enumerate}
\end{flashcard}


\begin{flashcard}
    {Define conditional entropy and list some properties}

\end{flashcard}


\begin{flashcard}
    {Define mutual information and list some properties}

\end{flashcard}


\begin{flashcard}
    {Define the Kullback-Leibler divergence and list some properties}

\end{flashcard}


\begin{flashcard}
    {State the data processing inequality}

\end{flashcard}


\begin{flashcard}
    {State Fano's inequality}
    
\end{flashcard}



\end{document}
