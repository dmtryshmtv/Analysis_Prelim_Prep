\documentclass[avery5388,grid,frame]{flashcards}

\cardfrontstyle[\large\slshape]{headings}
\cardbackstyle{empty}

\usepackage{amssymb, amsmath, amsfonts}
\usepackage{mathtools}
\usepackage{physics}
\usepackage{enumerate}
\usepackage{array}

\newcommand{\E}{\varepsilon}
\newcommand{\ran}{\mathrm{ran}\,}
\newcommand{\ind}{\mathrm{ind}\,}
\newcommand{\sgn}[1]{\mathrm{sgn}\left[#1\right]}
\newcommand{\f}[3]{#1\ :\ #2 \rightarrow #3}
\def\Rl{\mathbb{R}}
\def\Cx{\mathbb{C}}
\def\hilb{\mathcal{H}}
\def\torus{\mathbb{T}}

\begin{document}

\cardfrontfoot{Applied Analysis Chapter 9}


\begin{flashcard}
    {What is an eigenvalue?  What is an eigenvector?  What does it mean for a matrix to be diagonalizable?  What does the spectrum of a finite-dimensional linear operator consist of?}
    Consider an $n\times n$ matrix $A$ with complex entries.  A number $\lambda$ is an eigenvalue of $A$ if there is a nonzero vector $u$ such that
    \begin{align*}
        Au = \lambda u.
    \end{align*}
    If $\lambda$ is an eigenvector such that $Au = \lambda u$, then $u$ is called an eigenvector of $A$ corresponding to $\lambda$. \\

    A matrix $A$ is diagonalizable if there is a basis $\{u_1, \dots, u_n\}$ of $\Cx^n$ such that there are eigenvalues $\{\lambda_1, \dots, \lambda_n\}$, which may not be distinct, such that
    \begin{align*}
        A u_k = \lambda_k u_k, \qquad \text{for } k = 1, \dots, n.
    \end{align*}

    The spectrum of a finite-dimensional linear operator $A$, denoted $\sigma(A)$, consists of the eigenvalues of the matrix representing $A$.
\end{flashcard}

\begin{flashcard}
    {Describe the diagonalization procedure for operators on finite-dimensional Hilbert spaces.}
    Let $\{u_1, \dots, u_n\}$ be an orthonormal basis of $\Cx^n$.  Then construct the matrix $U = \qty(u_1\ u_2\ \dots\ u_n)$ where the columns of $U$ are the basis vectors.  Then denote $\{e_1, \dots, e_n\}$ as the standard basis of $\Cx^n$.  It follows that
    \begin{align*}
        Ue_k = u_k, \qquad U^*u_k = e_k, \qquad \text{i.e.~}U^* = U^{-1}
    \end{align*}
    Next, suppose the basis vectors $\{u_1, \dots, u_n\}$ are eigenvectors of $A$, i.e.~$\exists \lambda_k$ such that $Au_k = \lambda_k u_k$ for $k = 1, \dots, n$.  It then follows that
    \begin{align*}
        U^*AUe_k = \lambda_k e_k,
    \end{align*}
    which shows $D = U^*AU$ is a diagonal matrix with the eigenvalues on the diagonal, so $A = UDU^*$ where $D = (d_{ij})$ and $d_{ij} = \delta_{ij}\lambda_i$.

    Indeed, if $A = UDU^*$ with $U$ unitary and $D$ diagonal, then the columns of $U$ form an orthonormal basis of $\Cx^n$ consisting of eigenvectors of $A$.

    $A$ is a normal operator.
\end{flashcard}

\begin{flashcard}
    {When does a finite dimensional Hilbert space $\Cx^n$ have an orthonormal basis consisting of eigenvectors of an operator $A$?}
    An $n\times n$ complex matrix $A$ is normal if and only if $\Cx^n$ has an orthonormal basis consisting of eigenvectors of $A$.
\end{flashcard}

\begin{flashcard}
    {Describe the polar decomposition of a normal complex matrix $N$.}
    We want to decompose a normal matrix $N$ in to the product of a unitary matrix $V$ and a nonnegative matrix $A$. \\

    Since $N$ is normal, it can decomposition $N = UDU^*$ where $U$ is unitary and $D$ is diagonal ($D = (d_{ij})$ where $d_{ij} = \delta_{ij}\lambda_i$ and $\lambda_i$ are the eigenvalues of $N$).  We can rewrite $D$ as $D = \Phi\abs{D}$ where $\Phi$ is a diagonal matrix consisting of $\arg{\lambda_i}$ and $\abs{D}$ is a diagonal matrix consisting of $\abs{\lambda_i}$.  Then
    \begin{align*}
        N = VA, \qquad \text{where } V = U\Phi U^*\ \text{and}\ A = U\abs{D}U^*.
    \end{align*}
    Also, $A$ is non-negative, meaning $u^*Au \geq 0$ for all $u \in \Cx^n$. \\

    This is the matrix analog of the polar decomposition of a complex number $z = r e^{i\theta}$ in to the non-negative part $r$ and the complex number $e^{i\theta}$.
\end{flashcard}

\begin{flashcard}
    {What is the characteristic polynomial of a matrix?}
    The eigenvalues of a matrix $A$ are the roots of the characteristic polynomial $p_A$ of $A$, given by $p_A(\lambda) = \det(A - \lambda I)$.
\end{flashcard}

\begin{flashcard}
    {What does it mean if $p_A(\lambda) = 0$.}
    If $p_A(\lambda) = 0$, then $A - \lambda I$ is singular, and in particular, $\ker (A - \lambda I) \neq \{0\}$.  This means $\lambda$ is an eigenvector.
\end{flashcard}

\begin{flashcard}
    {What is the algebraic multiplicity of an eigenvalue?  What is the geometric multiplicity of an eigenvalue?}
    The algebraic multiplicity of an eigenvalue $\lambda$ is the power on the factor $(x - \lambda)$ in $p_A(\lambda)$. \\

    The geometric multiplicity is the dimension of the eigenspace associated with $\lambda$, that is, the dimension of $\ker (A - \lambda I)$. \\

    The geometric multiplicity of an eigenvalue is never greater than the algebraic multiplicity.
\end{flashcard}

\begin{flashcard}
    {Define the resolvent set and the spectrum of an operator $A \in \mathcal{B}(\hilb)$.}
    The resolvent set of an operator $A \in \mathcal{B}(\hilb)$, denoted $\rho(A)$, is the set of complex numbers such that $\f{\qty(A - \lambda I)}{\hilb}{\hilb}$ is one-to-one and onto. \\

    The spectrum of $A$, denoted $\sigma(A)$, is the complement of the resolvent set:
    \begin{align*}
        \sigma(A) = \Cx\setminus\rho(A).
    \end{align*}
\end{flashcard}

\begin{flashcard}
    {What are the three components of the spectrum?}
    \begin{enumerate}[(a)]
        \item The point spectrum of $A$ consists of all $\lambda \in \sigma(A)$ such that $A - \lambda I$ is not one-to-one.  In this case, $\ker(A - \lambda I) \neq \{0\}$, and $\lambda$ is called an eigenvalue of $A$.
        \item The continuous spectrum of $A$ consists of all $\lambda \in \sigma(A)$ such that $A - \lambda I$ is one-to-one but not onto, and $\ran(A - \lambda I)$ is dense in $\hilb$.
        \item The residual spectrum of $A$ consists of all $\lambda \in \sigma(A)$ such that $A - \lambda I$ is one-to-one but not onto, and $\ran(A - \lambda I)$ is not dense in $\hilb$.
    \end{enumerate}
\end{flashcard}

\begin{flashcard}
    {Define $M \in \mathcal{B}\qty(L^2\qty(\qty[0,1]))$ by $Mf(x) = xf(x)$.  Find and classiy the spectrum of $M$.}
    If $Mf = \lambda f$, then $(x - \lambda)f = 0$, and so $f = 0$.  Since eigenvectors cannot be $0$, then there are no eigenvalues of $M$.

    If $\lambda \not\in [0,1]$, then $(x - \lambda)f(x) \in L^2([0,1])$ since $x - \lambda$ is bounded away from $0$ on $[0,1]$.  Thus $\Cx \setminus [0,1] \in \rho(M)$. 

    If $\lambda \in [0,1]$, then $M - \lambda I$ is not onto since $f(x) \equiv c \in L^2$ but if $(M - \lambda I)g = f$ then $(x - \lambda) g(x) = c$, and so $g(x) = \frac{c}{x - \lambda}$, which is not an $L^2$ function.

    Let $f \in L^2$.  Then define $f_n \in L^2([0,1])$ by
    \begin{align*}
        f_n(x) = \mathcal{X}_{[B_{1/n}(\lambda)]^C}f(x).
    \end{align*}
    Then $f_n \in \ran (M - \lambda I)$ since $f_n(x) = \qty(M - \lambda I)\frac{f_n(x)}{x - \lambda}$ and $\frac{f_n(x)}{x - \lambda} \in L^2([0,1])$.  Also, $f_n \rightarrow f$ in $L^2([0,1])$.  Thus $\ran(M - \lambda I)$ is dense for all $\lambda \in [0,1]$.  Thus the spectrum of $M$ is completely continuous, and is $\sigma(M) = [0,1]$.
\end{flashcard}

\begin{flashcard}
    {Define the resolvent of $A$ at $\lambda$.}
    Suppose $\lambda \in \rho(A)$.  Then $(A - \lambda I)$ is invertible.  Define the resolvent of $A$ at $\lambda$, denoted $R(\lambda)$, by $R(\lambda) = (A - \lambda I)^{-1}$.  The resolvent is an operator valued function defined on $\rho(A)$, that is,
    \begin{align*}
        \f{R}{\rho(A)}{\mathcal{B}(\hilb)}.
    \end{align*}
\end{flashcard}

\begin{flashcard}
    {What are some basic properties of the resolvent set of $A$?  What does this say about the spectrum of $A$?  Define the spectral radius of $A$.}
    The resolvent set $\rho(A)$ of an operator $A$ on a Hilbert space $\hilb$ is open and contains the exterior disc $\lambda \in \Cx\ |\ \abs{\lambda} > \norm{A}$. \\

    Since the $\sigma(A) = \qty[\rho(A)]^C$, then $\sigma(A) \subset B_{\norm{A}}(0)$. \\

    The spectral radius of $A$, denoted $r(A)$, is the radius of the smallest disk which contains $\sigma(A)$, that is,
    \begin{align*}
        r(A) = \sup\left\{\abs{\lambda}\ |\ \lambda \in \sigma(A)\right\} = \inf\left\{r\ |\ \sigma(A) \subset B_r(0)\right\}
    \end{align*}
\end{flashcard}

\begin{flashcard}
    {What is a simple formula for the spectral radius of a bounded linear operator?  What happens if $A$ is self-adjoint?}
    For any bounded linear operator $A$,
    \begin{align*}
        r(A) = \lim_{n\rightarrow \infty} \norm{A^n}^{\frac{1}{n}}.
    \end{align*}

    When $A$ is self adjoint, $r(A) = \norm{A}$.
\end{flashcard}

\begin{flashcard}
    {Give an example of a matrix where $A$ where $\norm{A} > 0$ but $r(A) = 0$.}
    Let $A$ be the $n \times n$ matrix defined by
    \begin{equation*}
        N = \qty(\begin{array}{ccccc}
            0 & 1 & 0 & \dots & 0 \\
            0 & 0 & 1 & \dots & 0 \\
            \vdots & \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & 0 & \dots & 1 \\
            0 & 0 & 0 & \dots & 0
        \end{array})
    \end{equation*}
    Then $N^n = 0$, and thus $r(N) = \displaystyle\lim_{n\rightarrow \infty}\norm{N^n}^{\frac{1}{n}} = 0$, but $\norm{N} = 1$. \\

    $N$ is called nilpotent since $r(N) = 0$.
\end{flashcard}

\begin{flashcard}
    {Can the spectrum of a bounded linear operator be empty?}
    No.  $\sigma(A)$ must consist of at least a single point.
\end{flashcard}

% \begin{flashcard}
%     {Give an example of a bounded linear operator whose spectrum consists of a single point.}
%     Let $\f{K}{L^2([0,1])}{L^2([0,1])}$ be the Volterra integral operator given by $Kf(x) = \int_0^x f(y) \dd y$.  Then $K^*$ is given by $K^*f(x) = \int_x^1 f(y) \dd y$.
% \end{flashcard}

\begin{flashcard}
    {Describe the spectrum of compact, self-adjoint operators.}
    Let $K$ be a compact, self-adjoint operator with $K = K^*$.

    Then $\sigma(K)$ consists entirely of eigenvectors, except possibly $0$, which may belong to the continuous spectrum.
\end{flashcard}

\begin{flashcard}
    {Show that the eigenvalues of a bounded, self-adjoint linear operator are real, and eigenvalues associated with different eigenvectors are orthogonal.}
    Let $\lambda$ be an eigenvalue of $A$ with eigenvector $u$.  Then
    \begin{align*}
        \lambda(u,u) = (\lambda u,u) = (Au,u) = (u,Au) = (u,\lambda u) = \overline{\lambda}(u,u),
    \end{align*}
    which shows $\lambda = \overline{\lambda}$, i.e.~$\lambda \in \Rl$. \\

    Let $\lambda$ be an eigenvalue with eigenvector $u$ and $\mu$ be an eigenvalue with eigenvector $v$.  Then
    \begin{align*}
        \lambda(u,v) = (\lambda u,v) = (Au, v) = (u, Av) = (u, \mu v) = \overline{\mu}(u,v) = \mu(u,v).
    \end{align*}
    Thus, $(\lambda - \mu)(u,v) = 0$.  So if $\lambda \neq \mu$, then $(u,v) = 0$, which shows eigenvectors corresponding to different eigenvalues are orthogonal.
\end{flashcard}

\begin{flashcard}
    {What is an invariant subspace, and why are they important?}
    Let $M$ be a linear subspace of a Hilbert space $\hilb$.  Then $M$ is called an invariant subspace of a linear operator $A$ if
    \begin{align*}
        Ax \in M, \qquad \forall x \in M
    \end{align*}
    This means $A|_M$ is a linear operator on $M$. \\

    Suppose $M$ and $N$ are invariant subspaces of an operator $A$ with $\hilb = M \oplus N$.  Then each $x \in \hilb$ can be written uniquely as $x = m + n$ where $m \in M$ and $n \in N$.  Then
    \begin{align*}
        Ax = A|_Mm + A|_Nn.
    \end{align*}

    Invariant subspaces of an operator $A$ are important because the action of $A$ on $\hilb$ is completely determined by its actions on the invariant subspaces of $A$.
\end{flashcard}

\begin{flashcard}
    {Show that for bounded, self-adjoint operators, the orthogonal complement of an invariant subspace is invariant.}
    Let $M$ be an invariant subspace of the bounded, self-adjoint operator $A$ on a Hilbert space $\hilb$.  Choose an arbitrary $y \in M^{\perp}$.  Then $\forall x \in M$, $(x,y) = 0$.  Also, since $Ax \in M$, then $(Ax, y) = 0$ for all $x \in M$.  Then since $A$ is self-adjoint, $0 = (Ax,y) = (x,Ay)$, and thus $Ay \perp x$ for all $x \in M$, which shows $Ay \in M^\perp$.  Thus $M^\perp$ is an invariant subspace of $A$.
\end{flashcard}

\end{document}

